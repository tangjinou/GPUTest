run_name: OLMo-20M-local
seed: 42
dry_run: false

wandb:
  name: ${run_name}
  project: olmo-tiny-local

model:
  d_model: 256
  n_heads: 8
  n_layers: 8
  mlp_ratio: 8
  weight_tying: false
  alibi: false
  rope: true
  flash_attention: false
  attention_dropout: 0.0
  attention_layer_norm: false
  clip_qkv: null
  include_bias: false
  block_type: sequential
  layer_norm_type: rms
  layer_norm_with_affine: true
  layer_norm_eps: 1e-6
  bias_for_layer_norm: false
  attention_layer_norm_with_affine: false
  activation_type: swiglu
  residual_dropout: 0.0
  embedding_dropout: 0.0
  max_sequence_length: 2048
  vocab_size: 50280
  embedding_size: 50304
  eos_token_id: 0
  pad_token_id: 1
  init_device: cuda
  init_fn: normal
  init_std: 0.02
  init_cutoff_factor: 3

ddp:
  grad_sync_mode: batch
  find_unused_params: false

compile: null

optimizer:
  name: adamw
  learning_rate: 6.0e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1.0e-8

scheduler:
  name: cosine_with_warmup
  t_warmup: 100
  alpha_f: 0.1
  warmup_min_lr: 0

tokenizer:
  identifier: tokenizers/allenai_gpt-neox-olmo-dolma-v1_5.json
  truncate_direction: right

global_train_batch_size: 32
device_train_microbatch_size: 4
max_duration: 1000

precision: amp_bf16

data:
  pad_direction: right
  num_workers: 4
  drop_last: true
  pin_memory: true
  prefetch_factor: 8
  persistent_workers: true
  timeout: 0
  instance_filter:
    repetition_max_period: 13
    repetition_min_period: 1
    repetition_max_count: 32
  paths:
    # 使用本地测试数据文件
    - "file:///root/code/llm/OLMo/olmo_data/example_data/part-0-00000.npy"
    - "file:///root/code/llm/OLMo/olmo_data/example_data/part-1-00000.npy"
    - "file:///root/code/llm/OLMo/olmo_data/example_data/part-2-00000.npy"

# 分布式策略
distributed_strategy: "ddp"

# 检查点配置
save_folder: "workspace/OLMo-20M-local"
save_num_checkpoints_to_keep: 2
save_overwrite: true
remote_save_folder: null  # 明确设置为null，避免访问S3
load_path: null  # 明确设置为null，避免从S3加载
try_load_latest_save: false  # 禁用自动加载最新检查点，避免访问S3