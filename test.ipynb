{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f86036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰GPUæ•°é‡: 4\n"
     ]
    }
   ],
   "source": [
    "#æŸ¥è¯¢ä¸‹GPUçš„æ•°é‡å·²ç»å½“ä¸‹çš„æƒ…å†µ\n",
    "import torch\n",
    "\n",
    "# æŸ¥è¯¢GPUæ•°é‡\n",
    "print(f\"å½“å‰GPUæ•°é‡: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1373df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®¾å¤‡ 0:\n",
      " - åç§°: NVIDIA A100-SXM4-80GB\n",
      " - ä½¿ç”¨æƒ…å†µ: 0.00 GB å·²åˆ†é… / 0.00 GB è¢«é¢„ç•™\n",
      "è®¾å¤‡ 1:\n",
      " - åç§°: NVIDIA A100-SXM4-80GB\n",
      " - ä½¿ç”¨æƒ…å†µ: 0.00 GB å·²åˆ†é… / 0.00 GB è¢«é¢„ç•™\n",
      "è®¾å¤‡ 2:\n",
      " - åç§°: NVIDIA A100-SXM4-80GB\n",
      " - ä½¿ç”¨æƒ…å†µ: 0.00 GB å·²åˆ†é… / 0.00 GB è¢«é¢„ç•™\n",
      "è®¾å¤‡ 3:\n",
      " - åç§°: NVIDIA A100-SXM4-80GB\n",
      " - ä½¿ç”¨æƒ…å†µ: 0.00 GB å·²åˆ†é… / 0.00 GB è¢«é¢„ç•™\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥è¯¢å½“å‰GPUä½¿ç”¨æƒ…å†µ\n",
    "# Print GPU usage information\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"è®¾å¤‡ {i}:\")\n",
    "        print(f\" - åç§°: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\" - ä½¿ç”¨æƒ…å†µ: {torch.cuda.memory_allocated(i) / (1024 ** 3):.2f} GB å·²åˆ†é… / {torch.cuda.memory_reserved(i) / (1024 ** 3):.2f} GB è¢«é¢„ç•™\")\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å¯ç”¨çš„GPUã€‚\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1a4d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PyTorch å’Œ CUDA ç‰ˆæœ¬ä¿¡æ¯\n",
      "============================================================\n",
      "PyTorch ç‰ˆæœ¬: 2.9.1+cu128\n",
      "CUDA æ˜¯å¦å¯ç”¨: True\n",
      "CUDA ç‰ˆæœ¬: 12.8\n",
      "cuDNN ç‰ˆæœ¬: 91300\n",
      "cuDNN æ˜¯å¦å¯ç”¨: True\n",
      "\n",
      "GPU è®¾å¤‡ä¿¡æ¯:\n",
      "  è®¾å¤‡ 0: NVIDIA A100-SXM4-80GB\n",
      "    CUDA è®¡ç®—èƒ½åŠ›: (8, 0)\n",
      "    æ€»æ˜¾å­˜: 79.44 GB\n",
      "  è®¾å¤‡ 1: NVIDIA A100-SXM4-80GB\n",
      "    CUDA è®¡ç®—èƒ½åŠ›: (8, 0)\n",
      "    æ€»æ˜¾å­˜: 79.44 GB\n",
      "  è®¾å¤‡ 2: NVIDIA A100-SXM4-80GB\n",
      "    CUDA è®¡ç®—èƒ½åŠ›: (8, 0)\n",
      "    æ€»æ˜¾å­˜: 79.44 GB\n",
      "  è®¾å¤‡ 3: NVIDIA A100-SXM4-80GB\n",
      "    CUDA è®¡ç®—èƒ½åŠ›: (8, 0)\n",
      "    æ€»æ˜¾å­˜: 79.44 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹PyTorchå’ŒCUDAç‰ˆæœ¬\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch å’Œ CUDA ç‰ˆæœ¬ä¿¡æ¯\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"cuDNN æ˜¯å¦å¯ç”¨: {torch.backends.cudnn.enabled}\")\n",
    "    print(f\"\\nGPU è®¾å¤‡ä¿¡æ¯:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  è®¾å¤‡ {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    CUDA è®¡ç®—èƒ½åŠ›: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"    æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥CUDAå®‰è£…\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee156ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CPU ä¿¡æ¯\n",
      "============================================================\n",
      "å¤„ç†å™¨: x86_64\n",
      "æ¶æ„: x86_64\n",
      "å¹³å°: Linux-6.8.0-87-generic-x86_64-with-glibc2.35\n",
      "\n",
      "è¯¦ç»†CPUä¿¡æ¯:\n",
      "  model name\t: Intel(R) Xeon(R) CPU E5-2699C v4 @ 2.20GHz\n",
      "  cpu cores\t: 22\n",
      "  ç‰©ç†CPUæ•°é‡: 2\n",
      "  é€»è¾‘æ ¸å¿ƒæ•°: 88\n",
      "  cpu MHz\t\t: 1200.000\n",
      "\n",
      "Pythonæ£€æµ‹åˆ°çš„CPUæ ¸å¿ƒæ•°: 88\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹CPUå‹å·å’Œè¯¦ç»†ä¿¡æ¯\n",
    "import platform\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CPU ä¿¡æ¯\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ–¹æ³•1: ä½¿ç”¨platformæ¨¡å—\n",
    "print(f\"å¤„ç†å™¨: {platform.processor()}\")\n",
    "print(f\"æ¶æ„: {platform.machine()}\")\n",
    "print(f\"å¹³å°: {platform.platform()}\")\n",
    "\n",
    "# æ–¹æ³•2: è¯»å–/proc/cpuinfoè·å–è¯¦ç»†ä¿¡æ¯ï¼ˆLinuxç³»ç»Ÿï¼‰\n",
    "if os.path.exists('/proc/cpuinfo'):\n",
    "    print(\"\\nè¯¦ç»†CPUä¿¡æ¯:\")\n",
    "    with open('/proc/cpuinfo', 'r') as f:\n",
    "        cpuinfo = f.read()\n",
    "        # æå–CPUå‹å·\n",
    "        for line in cpuinfo.split('\\n'):\n",
    "            if 'model name' in line.lower():\n",
    "                print(f\"  {line.strip()}\")\n",
    "                break\n",
    "        # æå–CPUæ ¸å¿ƒæ•°\n",
    "        for line in cpuinfo.split('\\n'):\n",
    "            if 'cpu cores' in line.lower():\n",
    "                print(f\"  {line.strip()}\")\n",
    "                break\n",
    "        # æå–ç‰©ç†CPUæ•°é‡\n",
    "        physical_cpus = set()\n",
    "        for line in cpuinfo.split('\\n'):\n",
    "            if 'physical id' in line.lower():\n",
    "                physical_cpus.add(line.strip())\n",
    "        if physical_cpus:\n",
    "            print(f\"  ç‰©ç†CPUæ•°é‡: {len(physical_cpus)}\")\n",
    "        # æå–é€»è¾‘æ ¸å¿ƒæ•°\n",
    "        logical_cores = len([line for line in cpuinfo.split('\\n') if line.startswith('processor')])\n",
    "        print(f\"  é€»è¾‘æ ¸å¿ƒæ•°: {logical_cores}\")\n",
    "        # æå–CPUé¢‘ç‡\n",
    "        for line in cpuinfo.split('\\n'):\n",
    "            if 'cpu mhz' in line.lower() or 'bogomips' in line.lower():\n",
    "                print(f\"  {line.strip()}\")\n",
    "                break\n",
    "\n",
    "# æ–¹æ³•3: ä½¿ç”¨os.cpu_count()\n",
    "print(f\"\\nPythonæ£€æµ‹åˆ°çš„CPUæ ¸å¿ƒæ•°: {os.cpu_count()}\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce63eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰GPU: NVIDIA A100-SXM4-80GB\n",
      "å½“å‰GPU: NVIDIA A100-SXM4-80GB\n",
      "\n",
      "è®¾å¤‡ 1 (NVIDIA A100-SXM4-80GB) çš„å¼ é‡æ ¸å¿ƒåŠ é€Ÿèƒ½åŠ›:\n",
      "--------------------------------------------------------------------------------\n",
      "ç²¾åº¦           å®é™…å€¼             ç†è®ºå€¼             è¾¾æˆç‡          çŠ¶æ€        \n",
      "--------------------------------------------------------------------------------\n",
      "FP32          17.39 TFLOPS     19.50 TFLOPS    89.18%    âœ“ æ­£å¸¸\n",
      "TF32          98.48 TFLOPS    156.00 TFLOPS    63.13%    âœ“ æ­£å¸¸\n",
      "FP16         245.83 TFLOPS    312.00 TFLOPS    78.79%    âœ“ æ­£å¸¸\n",
      "BFLOAT16     229.10 TFLOPS    312.00 TFLOPS    73.43%    âœ“ æ­£å¸¸\n",
      "ğŸ“ è¯´æ˜:\n",
      "   â€¢ è¾¾æˆç‡åŸºäºç†è®ºå³°å€¼ç®—åŠ›ï¼ŒPyTorchå®é™…æ€§èƒ½é€šå¸¸åœ¨ç†è®ºå€¼çš„60-85%èŒƒå›´å†…æ˜¯æ­£å¸¸çš„\n",
      "   â€¢ FP32: æ ‡å‡†å•ç²¾åº¦æµ®ç‚¹æ•°ï¼Œä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒï¼Œæ€§èƒ½è¾ƒä½ä½†ç²¾åº¦æœ€é«˜\n",
      "   â€¢ TF32: ä½¿ç”¨å¼ é‡æ ¸å¿ƒåŠ é€Ÿçš„FP32å˜ä½“ï¼Œæ€§èƒ½æ¯”FP32é«˜çº¦8å€\n",
      "   â€¢ ğŸ“š è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ä¸‹é¢çš„Markdownå•å…ƒæ ¼\n",
      "\n",
      "å½“å‰GPU: NVIDIA A100-SXM4-80GB\n",
      "å½“å‰GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# ä¿®å¤bugåçš„GPUå¼ é‡æ ¸å¿ƒåŠ é€Ÿç®—åŠ›æµ‹è¯•\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# åœ¨PyTorchä¸­å¯ç”¨TF32ï¼ˆå¯èƒ½é»˜è®¤å·²å¼€å¯ï¼Œå»ºè®®ç¡®è®¤ï¼‰\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def get_theoretical_flops(gpu_name):\n",
    "    \"\"\"\n",
    "    æ ¹æ®GPUå‹å·è¿”å›ç†è®ºç®—åŠ›å€¼\n",
    "    \"\"\"\n",
    "    gpu_name_upper = gpu_name.upper()\n",
    "    # A100ç³»åˆ—ç†è®ºå€¼\n",
    "    if 'A100' in gpu_name_upper:\n",
    "        return {\n",
    "            'FP32': 19.5,       # TFLOPS (ä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒï¼Œæ ‡å‡†FP32)\n",
    "            'TF32': 156.0,      # TFLOPS (ä½¿ç”¨å¼ é‡æ ¸å¿ƒ)\n",
    "            'FP16': 312.0,      # TFLOPS\n",
    "            'BFLOAT16': 312.0,  # TFLOPS\n",
    "            'INT8': 624.0       # TOPS\n",
    "        }\n",
    "    # H100ç³»åˆ—ç†è®ºå€¼\n",
    "    elif 'H100' in gpu_name_upper:\n",
    "        return {\n",
    "            'FP32': 67.0,       # TFLOPS (ä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒï¼Œæ ‡å‡†FP32)\n",
    "            'TF32': 1000.0,     # TFLOPS (ä½¿ç”¨å¼ é‡æ ¸å¿ƒ)\n",
    "            'FP16': 2000.0,     # TFLOPS\n",
    "            'BFLOAT16': 2000.0, # TFLOPS\n",
    "            'INT8': 4000.0      # TOPS\n",
    "        }\n",
    "    # V100ç³»åˆ—ç†è®ºå€¼\n",
    "    elif 'V100' in gpu_name_upper:\n",
    "        return {\n",
    "            'FP32': 15.7,       # TFLOPS (ä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒï¼Œæ ‡å‡†FP32)\n",
    "            'TF32': 0.0,        # V100ä¸æ”¯æŒTF32\n",
    "            'FP16': 125.0,      # TFLOPS\n",
    "            'BFLOAT16': 0.0,    # V100ä¸æ”¯æŒBFLOAT16\n",
    "            'INT8': 0.0         # V100ä¸æ”¯æŒINT8å¼ é‡æ ¸å¿ƒ\n",
    "        }\n",
    "    # RTX 3090/4090ç­‰æ¶ˆè´¹çº§GPU\n",
    "    elif 'RTX 3090' in gpu_name_upper or 'RTX 4090' in gpu_name_upper:\n",
    "        if '4090' in gpu_name_upper:\n",
    "            return {\n",
    "                'FP32': 83.0,       # TFLOPS (ä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒï¼Œæ ‡å‡†FP32)\n",
    "                'TF32': 0.0,        # RTX 4090ä¸æ”¯æŒTF32\n",
    "                'FP16': 165.0,      # TFLOPS (çº¦)\n",
    "                'BFLOAT16': 165.0,  # TFLOPS (çº¦)\n",
    "                'INT8': 1320.0      # TOPS (çº¦)\n",
    "            }\n",
    "        else:  # RTX 3090\n",
    "            return {\n",
    "                'FP32': 36.0,       # TFLOPS (ä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒï¼Œæ ‡å‡†FP32)\n",
    "                'TF32': 0.0,        # RTX 3090ä¸æ”¯æŒTF32\n",
    "                'FP16': 142.0,      # TFLOPS (çº¦)\n",
    "                'BFLOAT16': 142.0,  # TFLOPS (çº¦)\n",
    "                'INT8': 1140.0      # TOPS (çº¦)\n",
    "            }\n",
    "    # é»˜è®¤å€¼ï¼ˆæœªçŸ¥GPUå‹å·ï¼‰\n",
    "    else:\n",
    "        return {\n",
    "            'FP32': 0.0,\n",
    "            'TF32': 0.0,\n",
    "            'FP16': 0.0,\n",
    "            'BFLOAT16': 0.0,\n",
    "            'INT8': 0.0\n",
    "        }\n",
    "\n",
    "def benchmark_gpu_flops(device_id, dtype, warmup_iterations=10, test_iterations=100, size=4096):\n",
    "    \"\"\"\n",
    "    æµ‹è¯•GPUåœ¨ä¸åŒç²¾åº¦ä¸‹çš„å®é™…ç®—åŠ›\n",
    "\n",
    "    å‚æ•°:\n",
    "        device_id: GPUè®¾å¤‡ID\n",
    "        dtype: æ•°æ®ç±»å‹ (torch.float32, torch.float16, torch.bfloat16)\n",
    "        warmup_iterations: é¢„çƒ­è¿­ä»£æ¬¡æ•°\n",
    "        test_iterations: æµ‹è¯•è¿­ä»£æ¬¡æ•°\n",
    "        size: square matrix sizeï¼Œç”¨äºæ€§èƒ½å¯¹æ¯”ï¼Œé»˜è®¤4096\n",
    "    è¿”å›:\n",
    "        å®é™…æµ‹å¾—çš„TFLOPS/TOPS\n",
    "    \"\"\"\n",
    "    device = torch.device(f'cuda:{device_id}')\n",
    "    torch.cuda.set_device(device)\n",
    "    # æ„é€ ä¸¤ä¸ªçŸ©é˜µ\n",
    "    if dtype == torch.int8:\n",
    "        # INT8ä¸åŒæµ‹è¯•æ–¹å¼ï¼Œæ­¤å¤„æš‚ä¸æ”¯æŒå¦‚éœ€æ±‚å†æ·»åŠ \n",
    "        raise NotImplementedError('INT8æš‚æœªå®ç°åŸºå‡†æµ‹è¯•')\n",
    "    else:\n",
    "        a = torch.randn(size, size, device=device, dtype=dtype)\n",
    "        b = torch.randn(size, size, device=device, dtype=dtype)\n",
    "\n",
    "    # é¢„çƒ­\n",
    "    torch.cuda.synchronize(device)\n",
    "    for _ in range(warmup_iterations):\n",
    "        c = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize(device)\n",
    "\n",
    "    # æ­£å¼æµ‹è¯•\n",
    "    torch.cuda.synchronize(device)\n",
    "    start_time = time.time()\n",
    "    for _ in range(test_iterations):\n",
    "        c = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize(device)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # FLOPè®¡ç®—å…¬å¼ï¼ŒçŸ©é˜µä¹˜æ³•: 2*N^3æ¯ä¸€æ¬¡\n",
    "    total_ops = 2 * (size ** 3) * test_iterations\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    tflops = (total_ops / elapsed_time) / 1e12\n",
    "    return tflops\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"å½“å‰GPU: {gpu_name}\")\n",
    "        #ç”¨æ¥è¿‡æ»¤å¡çš„å·¥ä½œ\n",
    "        # if i == 0 or i == 2 or i == 3:\n",
    "        #     continue\n",
    "        theoretical = get_theoretical_flops(gpu_name)\n",
    "        print(f\"\\nè®¾å¤‡ {i} ({gpu_name}) çš„å¼ é‡æ ¸å¿ƒåŠ é€Ÿèƒ½åŠ›:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'ç²¾åº¦':<12} {'å®é™…å€¼':<15} {'ç†è®ºå€¼':<15} {'è¾¾æˆç‡':<12} {'çŠ¶æ€':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        # æµ‹è¯•FP32 (æ ‡å‡†å•ç²¾åº¦ï¼Œä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒ)\n",
    "        try:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = False\n",
    "            torch.backends.cudnn.allow_tf32 = False\n",
    "            tflops_fp32 = benchmark_gpu_flops(i, torch.float32)\n",
    "            if theoretical['FP32'] > 0:\n",
    "                ratio = (tflops_fp32 / theoretical['FP32']) * 100\n",
    "                status = \"âœ“ æ­£å¸¸\" if ratio >= 50 else \"âš  åä½\"\n",
    "                print(f\"FP32       {tflops_fp32:>8.2f} TFLOPS  {theoretical['FP32']:>8.2f} TFLOPS  {ratio:>7.2f}%    {status}\")\n",
    "            else:\n",
    "                print(f\"FP32       {tflops_fp32:>8.2f} TFLOPS  {'ä¸æ”¯æŒ':<15} {'N/A':<12} {'-':<10}\")\n",
    "        except Exception as e:\n",
    "            print(f\"FP32       æµ‹è¯•å¤±è´¥ ({e})\")\n",
    "\n",
    "        # æµ‹è¯•TF32 (éœ€torch>=1.7; é€šè¿‡torch.float32, PyTorchä¼šè‡ªåŠ¨ç”¨TF32)\n",
    "        try:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            tflops_tf32 = benchmark_gpu_flops(i, torch.float32)\n",
    "            if theoretical['TF32'] > 0:\n",
    "                ratio = (tflops_tf32 / theoretical['TF32']) * 100\n",
    "                status = \"âœ“ æ­£å¸¸\" if ratio >= 50 else \"âš  åä½\"\n",
    "                print(f\"TF32       {tflops_tf32:>8.2f} TFLOPS  {theoretical['TF32']:>8.2f} TFLOPS  {ratio:>7.2f}%    {status}\")\n",
    "            else:\n",
    "                print(f\"TF32       {tflops_tf32:>8.2f} TFLOPS  {'ä¸æ”¯æŒ':<15} {'N/A':<12} {'-':<10}\")\n",
    "        except Exception as e:\n",
    "            print(f\"TF32       æµ‹è¯•å¤±è´¥ ({e})\")\n",
    "        # æµ‹è¯•FP16\n",
    "        try:\n",
    "            tflops_fp16 = benchmark_gpu_flops(i, torch.float16)\n",
    "            if theoretical['FP16'] > 0:\n",
    "                ratio = (tflops_fp16 / theoretical['FP16']) * 100\n",
    "                status = \"âœ“ æ­£å¸¸\" if ratio >= 50 else \"âš  åä½\"\n",
    "                print(f\"FP16       {tflops_fp16:>8.2f} TFLOPS  {theoretical['FP16']:>8.2f} TFLOPS  {ratio:>7.2f}%    {status}\")\n",
    "            else:\n",
    "                print(f\"FP16       {tflops_fp16:>8.2f} TFLOPS  {'ä¸æ”¯æŒ':<15} {'N/A':<12} {'-':<10}\")\n",
    "        except Exception as e:\n",
    "            print(f\"FP16       æµ‹è¯•å¤±è´¥ ({e})\")\n",
    "\n",
    "        # æµ‹è¯•BFLOAT16\n",
    "        try:\n",
    "            tflops_bf16 = benchmark_gpu_flops(i, torch.bfloat16)\n",
    "            if theoretical['BFLOAT16'] > 0:\n",
    "                ratio = (tflops_bf16 / theoretical['BFLOAT16']) * 100\n",
    "                status = \"âœ“ æ­£å¸¸\" if ratio >= 50 else \"âš  åä½\"\n",
    "                print(f\"BFLOAT16   {tflops_bf16:>8.2f} TFLOPS  {theoretical['BFLOAT16']:>8.2f} TFLOPS  {ratio:>7.2f}%    {status}\")\n",
    "            else:\n",
    "                print(f\"BFLOAT16   {tflops_bf16:>8.2f} TFLOPS  {'ä¸æ”¯æŒ':<15} {'N/A':<12} {'-':<10}\")\n",
    "        except Exception as e:\n",
    "            print(f\"BFLOAT16   æµ‹è¯•å¤±è´¥ ({e})\")\n",
    "\n",
    "     \n",
    "        print(\"ğŸ“ è¯´æ˜:\")\n",
    "        print(\"   â€¢ è¾¾æˆç‡åŸºäºç†è®ºå³°å€¼ç®—åŠ›ï¼ŒPyTorchå®é™…æ€§èƒ½é€šå¸¸åœ¨ç†è®ºå€¼çš„60-85%èŒƒå›´å†…æ˜¯æ­£å¸¸çš„\")\n",
    "        print(\"   â€¢ FP32: æ ‡å‡†å•ç²¾åº¦æµ®ç‚¹æ•°ï¼Œä¸ä½¿ç”¨å¼ é‡æ ¸å¿ƒï¼Œæ€§èƒ½è¾ƒä½ä½†ç²¾åº¦æœ€é«˜\")\n",
    "        print(\"   â€¢ TF32: ä½¿ç”¨å¼ é‡æ ¸å¿ƒåŠ é€Ÿçš„FP32å˜ä½“ï¼Œæ€§èƒ½æ¯”FP32é«˜çº¦8å€\")\n",
    "        print(\"   â€¢ ğŸ“š è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ä¸‹é¢çš„Markdownå•å…ƒæ ¼\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å¯ç”¨çš„GPUã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f291471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "æµ‹è¯•è·¨GPUæ‹·è´é€Ÿåº¦: ä»GPU0 --> GPU1 (1 GB, 10æ¬¡)\n",
      "å¹³å‡è€—æ—¶: 4.00 ms\n",
      "å¸¦å®½: 250.11 GB/s\n",
      "================================================================================\n",
      "\n",
      "æµ‹è¯•å¤šå¡å…¨äº’é€šå¸¦å®½ï¼ˆæ‰€æœ‰æ˜¾å¡ä¹‹é—´ä¸¤ä¸¤æ‹·è´ï¼‰:\n",
      "  GPU0 â†’ GPU1: 229.94 GB/s (avg 0.54 ms)\n",
      "  GPU0 â†’ GPU2: 10.39 GB/s (avg 12.03 ms)\n",
      "  GPU0 â†’ GPU3: 10.40 GB/s (avg 12.02 ms)\n",
      "  GPU1 â†’ GPU0: 230.90 GB/s (avg 0.54 ms)\n",
      "  GPU1 â†’ GPU2: 15.64 GB/s (avg 7.99 ms)\n",
      "  GPU1 â†’ GPU3: 10.31 GB/s (avg 12.12 ms)\n",
      "  GPU2 â†’ GPU0: 10.06 GB/s (avg 12.42 ms)\n",
      "  GPU2 â†’ GPU1: 15.74 GB/s (avg 7.94 ms)\n",
      "  GPU2 â†’ GPU3: 1083.24 GB/s (avg 0.12 ms)\n",
      "  GPU3 â†’ GPU0: 10.39 GB/s (avg 12.03 ms)\n",
      "  GPU3 â†’ GPU1: 15.73 GB/s (avg 7.95 ms)\n",
      "  GPU3 â†’ GPU2: 1507.06 GB/s (avg 0.08 ms)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def test_inter_gpu_bandwidth(size_gb=1, num_iters=10):\n",
    "    \"\"\"æµ‹è¯•å¤šå¼ GPUä¹‹é—´çš„å¸¦å®½å’Œå»¶è¿Ÿ\n",
    "    Args:\n",
    "        size_gb (int): æ¯æ¬¡æ‹·è´çš„æ•°æ®é‡ï¼Œå•ä½GB\n",
    "        num_iters (int): æ‹·è´æ¬¡æ•°\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() < 2:\n",
    "        print(\"éœ€è¦è‡³å°‘ä¸¤å¼ GPUè¿›è¡Œæµ‹è¯•ã€‚\")\n",
    "        return\n",
    "\n",
    "    src = 0\n",
    "    dst = 1\n",
    "    size = int(size_gb * 1024**3 // 4)  # float32, 4B\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"æµ‹è¯•è·¨GPUæ‹·è´é€Ÿåº¦: ä»GPU{src} --> GPU{dst} ({size_gb} GB, {num_iters}æ¬¡)\")\n",
    "\n",
    "    # åˆ›å»ºå¼ é‡\n",
    "    tensor_src = torch.randn(size, dtype=torch.float32, device=f'cuda:{src}')\n",
    "    torch.cuda.synchronize()\n",
    "    times = []\n",
    "    for _ in range(num_iters):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        tensor_dst = tensor_src.to(f'cuda:{dst}', non_blocking=False)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = sum(times[1:]) / (num_iters - 1)  # æ’é™¤é¦–æ¬¡warmup\n",
    "    bandwidth = size_gb / avg_time\n",
    "    print(f\"å¹³å‡è€—æ—¶: {avg_time*1000:.2f} ms\")\n",
    "    print(f\"å¸¦å®½: {bandwidth:.2f} GB/s\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # å¤šå¡å…¨äº’é€šæ¨¡å¼\n",
    "    print(f\"æµ‹è¯•å¤šå¡å…¨äº’é€šå¸¦å®½ï¼ˆæ‰€æœ‰æ˜¾å¡ä¹‹é—´ä¸¤ä¸¤æ‹·è´ï¼‰:\")\n",
    "    results = []\n",
    "    for src in range(torch.cuda.device_count()):\n",
    "        for dst in range(torch.cuda.device_count()):\n",
    "            if src == dst:\n",
    "                continue\n",
    "            tensor_src = torch.randn(size//8, dtype=torch.float32, device=f'cuda:{src}')\n",
    "            torch.cuda.synchronize()\n",
    "            times = []\n",
    "            for _ in range(num_iters):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                tensor_dst = tensor_src.to(f'cuda:{dst}', non_blocking=False)\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                times.append(end - start)\n",
    "            avg_time = sum(times[1:]) / (num_iters - 1)\n",
    "            bandwidth = (size_gb/8) / avg_time\n",
    "            results.append((src, dst, bandwidth))\n",
    "            print(f\"  GPU{src} â†’ GPU{dst}: {bandwidth:.2f} GB/s (avg {avg_time*1000:.2f} ms)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    return results\n",
    "\n",
    "# åªæœ‰æœ‰å¤šäºä¸€å¼ å¡æ—¶æ‰æµ‹è¯•\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    test_inter_gpu_bandwidth()\n",
    "else:\n",
    "    print(\"æœ¬æœºæ²¡æœ‰å¤šå¼ GPUï¼Œè·³è¿‡è·¨å¡å¸¦å®½æµ‹è¯•ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6c5c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPUæ‹“æ‰‘å’ŒP2Pè¿æ¥åˆ†æ\n",
      "================================================================================\n",
      "\n",
      "æ£€æµ‹åˆ° 4 ä¸ªGPUè®¾å¤‡\n",
      "\n",
      "1. GPUä¹‹é—´çš„P2P (Peer-to-Peer) è®¿é—®èƒ½åŠ›:\n",
      "--------------------------------------------------------------------------------\n",
      "  GPU0 â†’ GPU1: âœ“ æ”¯æŒ\n",
      "  GPU0 â†’ GPU2: âœ— ä¸æ”¯æŒ\n",
      "  GPU0 â†’ GPU3: âœ— ä¸æ”¯æŒ\n",
      "  GPU1 â†’ GPU0: âœ“ æ”¯æŒ\n",
      "  GPU1 â†’ GPU2: âœ— ä¸æ”¯æŒ\n",
      "  GPU1 â†’ GPU3: âœ— ä¸æ”¯æŒ\n",
      "  GPU2 â†’ GPU0: âœ— ä¸æ”¯æŒ\n",
      "  GPU2 â†’ GPU1: âœ— ä¸æ”¯æŒ\n",
      "  GPU2 â†’ GPU3: âœ“ æ”¯æŒ\n",
      "  GPU3 â†’ GPU0: âœ— ä¸æ”¯æŒ\n",
      "  GPU3 â†’ GPU1: âœ— ä¸æ”¯æŒ\n",
      "  GPU3 â†’ GPU2: âœ“ æ”¯æŒ\n",
      "\n",
      "2. ä½¿ç”¨nvidia-smiæŸ¥çœ‹GPUæ‹“æ‰‘:\n",
      "--------------------------------------------------------------------------------\n",
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tNV12\tSYS\tSYS\t0-21,44-65\t0\t\tN/A\n",
      "GPU1\tNV12\t X \tSYS\tSYS\t0-21,44-65\t0\t\tN/A\n",
      "GPU2\tSYS\tSYS\t X \tNV12\t22-43,66-87\t1\t\tN/A\n",
      "GPU3\tSYS\tSYS\tNV12\t X \t22-43,66-87\t1\t\tN/A\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n",
      "\n",
      "\n",
      "3. æ‹“æ‰‘è¿æ¥ç±»å‹è§£æ:\n",
      "--------------------------------------------------------------------------------\n",
      "æ ¹æ®nvidia-smi topoè¾“å‡º:\n",
      "  â€¢ PIX: é€šè¿‡å•ä¸ªPCIeæ¡¥æ¥ï¼ˆåŒä¸€PCIeåŸŸå†…ï¼Œè¾ƒå¿«ï¼‰\n",
      "  â€¢ SYS: é€šè¿‡PCIe + CPUäº’è¿ï¼ˆè·¨NUMAèŠ‚ç‚¹ï¼Œå¾ˆæ…¢ï¼‰\n",
      "  â€¢ NV#: é€šè¿‡NVLinkç›´è¿ï¼ˆæœ€å¿«ï¼Œä½†æ‚¨çš„ç³»ç»Ÿæœªæ˜¾ç¤ºï¼‰\n",
      "\n",
      "\n",
      "4. PCIeé…ç½®ä¿¡æ¯åˆ†æ:\n",
      "--------------------------------------------------------------------------------\n",
      "PCIeé“¾è·¯ä¿¡æ¯:\n",
      "  GPU0: PCIeæ€»çº¿=00000000:04:00.0, ç‰ˆæœ¬=Gen3, å®½åº¦=x16\n",
      "  GPU1: PCIeæ€»çº¿=00000000:05:00.0, ç‰ˆæœ¬=Gen3, å®½åº¦=x16\n",
      "  GPU2: PCIeæ€»çº¿=00000000:85:00.0, ç‰ˆæœ¬=Gen3, å®½åº¦=x16\n",
      "  GPU3: PCIeæ€»çº¿=00000000:86:00.0, ç‰ˆæœ¬=Gen3, å®½åº¦=x16\n",
      "\n",
      "5. æ— NVLinkæƒ…å†µä¸‹çš„é€Ÿåº¦å·®å¼‚åˆ†æ:\n",
      "--------------------------------------------------------------------------------\n",
      "æ‚¨çš„ç³»ç»Ÿæ‹“æ‰‘ç»“æ„:\n",
      "  â€¢ GPU0å’ŒGPU1: åŒä¸€NUMAèŠ‚ç‚¹ï¼ˆèŠ‚ç‚¹0ï¼‰ï¼Œæœ‰P2Pæ”¯æŒ\n",
      "  â€¢ GPU2å’ŒGPU3: åŒä¸€NUMAèŠ‚ç‚¹ï¼ˆèŠ‚ç‚¹1ï¼‰ï¼Œæœ‰P2Pæ”¯æŒ\n",
      "  â€¢ ç³»ç»Ÿæ— NVLinkè¿æ¥\n",
      "\n",
      "é€Ÿåº¦å·®å¼‚çš„æ ¹æœ¬åŸå› ï¼ˆæ— NVLinkï¼‰:\n",
      "\n",
      "  âœ“ GPU0 â†” GPU1: é€Ÿåº¦å¿«ï¼ˆ~1000+ GB/sï¼‰\n",
      "    â†’ æœ‰P2Pæ”¯æŒï¼Œé€šè¿‡PCIe P2Pé€šä¿¡\n",
      "    â†’ å¯èƒ½çš„åŸå› :\n",
      "       1. åŒä¸€PCIeæ ¹å¤åˆä½“ï¼ˆRoot Complexï¼‰ä¸‹\n",
      "       2. é€šè¿‡PCIeäº¤æ¢æœºç›´æ¥è¿æ¥ï¼ˆPIXè¿æ¥ï¼‰\n",
      "       3. PCIe 4.0 x16é…ç½®ï¼Œç†è®ºå¸¦å®½64 GB/sï¼ˆåŒå‘128 GB/sï¼‰\n",
      "       4. æ•°æ®ç›´æ¥åœ¨PCIeåŸŸå†…ä¼ è¾“ï¼Œæ— éœ€ç»è¿‡CPUå†…å­˜\n",
      "    â†’ è™½ç„¶æ— NVLinkï¼Œä½†PCIe P2Pä»ç„¶å¯ä»¥å®ç°è¾ƒé«˜é€Ÿåº¦\n",
      "\n",
      "  âš  GPU2 â†” GPU3: é€Ÿåº¦è¾ƒæ…¢ï¼ˆå³ä½¿æœ‰P2Pæ”¯æŒï¼‰\n",
      "    â†’ è™½ç„¶æœ‰P2Pæ”¯æŒï¼Œä½†é€Ÿåº¦ä»ç„¶è¾ƒæ…¢ï¼Œå¯èƒ½çš„åŸå› :\n",
      "       1. ä¸åŒçš„PCIeæ ¹å¤åˆä½“é…ç½®\n",
      "       2. PCIeç‰ˆæœ¬è¾ƒä½ï¼ˆå¦‚PCIe 3.0 vs 4.0ï¼‰\n",
      "       3. PCIeé€šé“æ•°è¾ƒå°‘ï¼ˆå¦‚x8 vs x16ï¼‰\n",
      "       4. éœ€è¦é€šè¿‡é¢å¤–çš„PCIeæ¡¥æ¥èŠ¯ç‰‡ï¼Œå¢åŠ å»¶è¿Ÿ\n",
      "       5. PCIeæ’æ§½ç±»å‹ä¸åŒï¼ˆå¦‚x16ç‰©ç†ä½†x8ç”µæ°”ï¼‰\n",
      "    â†’ æ£€æŸ¥PCIeé…ç½®:\n",
      "       GPU2: PCIe Gen3 x16\n",
      "       GPU3: PCIe Gen3 x16\n",
      "       GPU0: PCIe Gen3 x16\n",
      "       GPU1: PCIe Gen3 x16\n",
      "\n",
      "    â†’ å¯¹æ¯”åˆ†æ:\n",
      "\n",
      "  âš  GPU0/1 â†” GPU2/3: 1-15 GB/sï¼ˆè·¨NUMAèŠ‚ç‚¹é€šä¿¡ï¼‰\n",
      "    â†’ æ— P2Pæ”¯æŒï¼Œå¿…é¡»é€šè¿‡CPUå’ŒPCIe\n",
      "    â†’ éœ€è¦ç»è¿‡ä»¥ä¸‹è·¯å¾„:\n",
      "       1. GPU0 â†’ PCIe â†’ CPU0å†…å­˜ (NUMAèŠ‚ç‚¹0)\n",
      "       2. CPU0å†…å­˜ â†’ QPI/UPIäº’è¿ â†’ CPU1å†…å­˜ (NUMAèŠ‚ç‚¹1)\n",
      "       3. CPU1å†…å­˜ â†’ PCIe â†’ GPU2\n",
      "    â†’ è¿™ä¸ªè·¯å¾„æ¶‰åŠå¤šæ¬¡æ•°æ®æ‹·è´å’ŒCPUå‚ä¸ï¼Œå¯¼è‡´:\n",
      "       â€¢ é«˜å»¶è¿Ÿï¼ˆå¤šæ¬¡å†…å­˜æ‹·è´ï¼‰\n",
      "       â€¢ ä½å¸¦å®½ï¼ˆå—CPUäº’è¿å¸¦å®½é™åˆ¶ï¼Œé€šå¸¸<20 GB/sï¼‰\n",
      "    â†’ è¿™æ˜¯å…¸å‹çš„NUMAæ¶æ„é™åˆ¶\n",
      "\n",
      "6. GPUè®¾å¤‡å±æ€§:\n",
      "--------------------------------------------------------------------------------\n",
      "  GPU0:\n",
      "    åç§°: NVIDIA A100-SXM4-80GB\n",
      "    PCIæ€»çº¿ID: 4\n",
      "    PCIè®¾å¤‡ID: 0\n",
      "    å¤šå¤„ç†å™¨æ•°é‡: 108\n",
      "  GPU1:\n",
      "    åç§°: NVIDIA A100-SXM4-80GB\n",
      "    PCIæ€»çº¿ID: 5\n",
      "    PCIè®¾å¤‡ID: 0\n",
      "    å¤šå¤„ç†å™¨æ•°é‡: 108\n",
      "  GPU2:\n",
      "    åç§°: NVIDIA A100-SXM4-80GB\n",
      "    PCIæ€»çº¿ID: 133\n",
      "    PCIè®¾å¤‡ID: 0\n",
      "    å¤šå¤„ç†å™¨æ•°é‡: 108\n",
      "  GPU3:\n",
      "    åç§°: NVIDIA A100-SXM4-80GB\n",
      "    PCIæ€»çº¿ID: 134\n",
      "    PCIè®¾å¤‡ID: 0\n",
      "    å¤šå¤„ç†å™¨æ•°é‡: 108\n",
      "\n",
      "7. é’ˆå¯¹æ— NVLinkç³»ç»Ÿçš„ä¼˜åŒ–å»ºè®®:\n",
      "--------------------------------------------------------------------------------\n",
      "åŸºäºNUMAæ‹“æ‰‘çš„ä¼˜åŒ–ç­–ç•¥:\n",
      "\n",
      "  1. æ•°æ®å¹¶è¡Œè®­ç»ƒ:\n",
      "     â€¢ å°†æ¨¡å‹å‰¯æœ¬æ”¾åœ¨åŒä¸€NUMAèŠ‚ç‚¹å†…ï¼ˆGPU0+GPU1 æˆ– GPU2+GPU3ï¼‰\n",
      "     â€¢ é¿å…è·¨NUMAèŠ‚ç‚¹çš„é¢‘ç¹é€šä¿¡\n",
      "     â€¢ ä½¿ç”¨DataParallelæ—¶ï¼Œè®¾ç½®device_ids=[0,1]æˆ–[2,3]\n",
      "\n",
      "  2. æ¨¡å‹å¹¶è¡Œè®­ç»ƒ:\n",
      "     â€¢ å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼Œå°†ä¸åŒå±‚æ”¾åœ¨åŒä¸€NUMAèŠ‚ç‚¹çš„GPUä¸Š\n",
      "     â€¢ ä¾‹å¦‚ï¼šå‰å‡ å±‚åœ¨GPU0ï¼Œåå‡ å±‚åœ¨GPU1ï¼ˆéƒ½åœ¨èŠ‚ç‚¹0ï¼‰\n",
      "     â€¢ é¿å…å°†æ¨¡å‹åˆ†å‰²åˆ°è·¨NUMAèŠ‚ç‚¹çš„GPUä¸Š\n",
      "\n",
      "  3. æ··åˆç²¾åº¦å’Œé€šä¿¡ä¼˜åŒ–:\n",
      "     â€¢ ä½¿ç”¨æ¢¯åº¦å‹ç¼©ï¼ˆå¦‚FP16ï¼‰å‡å°‘è·¨NUMAé€šä¿¡çš„æ•°æ®é‡\n",
      "     â€¢ ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘é€šä¿¡é¢‘ç‡\n",
      "     â€¢ NCCLä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è·¯å¾„ï¼Œä½†è·¨NUMAä»ç„¶å¾ˆæ…¢\n",
      "\n",
      "  4. è¿›ç¨‹ç»‘å®š:\n",
      "     â€¢ ä½¿ç”¨numactlå°†è¿›ç¨‹ç»‘å®šåˆ°ç‰¹å®šNUMAèŠ‚ç‚¹\n",
      "     â€¢ ä¾‹å¦‚ï¼šnumactl --cpunodebind=0 --membind=0 python train.py\n",
      "     â€¢ è¿™å¯ä»¥é¿å…è·¨NUMAèŠ‚ç‚¹çš„å†…å­˜è®¿é—®\n",
      "\n",
      "  5. å¦‚æœå¿…é¡»ä½¿ç”¨æ‰€æœ‰4ä¸ªGPU:\n",
      "     â€¢ è€ƒè™‘ä½¿ç”¨2ä¸ªç‹¬ç«‹çš„è®­ç»ƒè¿›ç¨‹ï¼ˆæ¯ä¸ªä½¿ç”¨2ä¸ªGPUï¼‰\n",
      "     â€¢ æˆ–è€…æ¥å—è·¨NUMAé€šä¿¡çš„æ€§èƒ½æŸå¤±\n",
      "     â€¢ ä½¿ç”¨æ›´å¤§çš„batch sizeå’Œæ›´å°‘çš„é€šä¿¡é¢‘ç‡æ¥è¡¥å¿\n",
      "\n",
      "8. PCIe P2Pæ€§èƒ½è¯´æ˜:\n",
      "--------------------------------------------------------------------------------\n",
      "PCIe P2Pï¼ˆPeer-to-Peerï¼‰é€šä¿¡ç‰¹ç‚¹:\n",
      "  â€¢ å…è®¸GPUä¹‹é—´ç›´æ¥é€šè¿‡PCIeé€šä¿¡ï¼Œæ— éœ€ç»è¿‡CPUå†…å­˜\n",
      "  â€¢ æ€§èƒ½å–å†³äºPCIeé…ç½®:\n",
      "     - PCIe 4.0 x16: ç†è®ºå•å‘32 GB/sï¼ŒåŒå‘64 GB/s\n",
      "     - PCIe 3.0 x16: ç†è®ºå•å‘16 GB/sï¼ŒåŒå‘32 GB/s\n",
      "     - PCIe 4.0 x8:  ç†è®ºå•å‘16 GB/sï¼ŒåŒå‘32 GB/s\n",
      "     - PCIe 3.0 x8:  ç†è®ºå•å‘8 GB/sï¼ŒåŒå‘16 GB/s\n",
      "  â€¢ å®é™…æµ‹è¯•é€Ÿåº¦å¯èƒ½æ¥è¿‘ç†è®ºå€¼ï¼ˆå¦‚æœé…ç½®æ­£ç¡®ï¼‰\n",
      "  â€¢ GPU0 â†” GPU1é€Ÿåº¦å¿«ï¼Œè¯´æ˜PCIe P2Pé…ç½®è‰¯å¥½\n",
      "  â€¢ GPU2 â†” GPU3é€Ÿåº¦æ…¢ï¼Œå¯èƒ½æ˜¯PCIeé…ç½®ä¸åŒæˆ–å—é™\n",
      "\n",
      "æ£€æŸ¥PCIeé…ç½®å·®å¼‚çš„æ–¹æ³•:\n",
      "  â€¢ è¿è¡Œ: nvidia-smi --query-gpu=index,pcie.link.gen.current,pcie.link.width.current --format=csv\n",
      "  â€¢ æ£€æŸ¥BIOSä¸­çš„PCIeé…ç½®\n",
      "  â€¢ ç¡®è®¤æ‰€æœ‰GPUæ’æ§½éƒ½é…ç½®ä¸ºx16ï¼ˆå¦‚æœä¸»æ¿æ”¯æŒï¼‰\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥GPUæ‹“æ‰‘å’ŒP2Pè¿æ¥ï¼Œè§£é‡Šä¸ºä»€ä¹ˆGPU0å’ŒGPU1ä¹‹é—´å¿«ï¼Œå…¶ä»–æ…¢\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GPUæ‹“æ‰‘å’ŒP2Pè¿æ¥åˆ†æ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"\\næ£€æµ‹åˆ° {num_gpus} ä¸ªGPUè®¾å¤‡\\n\")\n",
    "    \n",
    "    # 1. æ£€æŸ¥PyTorchçš„P2Pè®¿é—®èƒ½åŠ›\n",
    "    print(\"1. GPUä¹‹é—´çš„P2P (Peer-to-Peer) è®¿é—®èƒ½åŠ›:\")\n",
    "    print(\"-\" * 80)\n",
    "    p2p_matrix = {}\n",
    "    for i in range(num_gpus):\n",
    "        for j in range(num_gpus):\n",
    "            if i != j:\n",
    "                can_access = torch.cuda.can_device_access_peer(i, j)\n",
    "                p2p_matrix[(i, j)] = can_access\n",
    "                status = \"âœ“ æ”¯æŒ\" if can_access else \"âœ— ä¸æ”¯æŒ\"\n",
    "                print(f\"  GPU{i} â†’ GPU{j}: {status}\")\n",
    "    \n",
    "    # 2. å°è¯•ä½¿ç”¨nvidia-smiæŸ¥çœ‹æ‹“æ‰‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "    print(\"\\n2. ä½¿ç”¨nvidia-smiæŸ¥çœ‹GPUæ‹“æ‰‘:\")\n",
    "    print(\"-\" * 80)\n",
    "    try:\n",
    "        # æ£€æŸ¥nvidia-smiæ˜¯å¦å¯ç”¨\n",
    "        result = subprocess.run(['nvidia-smi', 'topo', '-m'], \n",
    "                               capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"  nvidia-smi topoå‘½ä»¤ä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...\")\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):\n",
    "        print(\"  nvidia-smiå‘½ä»¤ä¸å¯ç”¨æˆ–è¶…æ—¶\")\n",
    "    \n",
    "    # 3. è§£ænvidia-smiæ‹“æ‰‘ä¿¡æ¯\n",
    "    print(\"\\n3. æ‹“æ‰‘è¿æ¥ç±»å‹è§£æ:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"æ ¹æ®nvidia-smi topoè¾“å‡º:\")\n",
    "    print(\"  â€¢ PIX: é€šè¿‡å•ä¸ªPCIeæ¡¥æ¥ï¼ˆåŒä¸€PCIeåŸŸå†…ï¼Œè¾ƒå¿«ï¼‰\")\n",
    "    print(\"  â€¢ SYS: é€šè¿‡PCIe + CPUäº’è¿ï¼ˆè·¨NUMAèŠ‚ç‚¹ï¼Œå¾ˆæ…¢ï¼‰\")\n",
    "    print(\"  â€¢ NV#: é€šè¿‡NVLinkç›´è¿ï¼ˆæœ€å¿«ï¼Œä½†æ‚¨çš„ç³»ç»Ÿæœªæ˜¾ç¤ºï¼‰\")\n",
    "    print()\n",
    "    \n",
    "    # åˆ†æNUMAæ‹“æ‰‘\n",
    "    numa_groups = {}\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', 'topo', '-m'], \n",
    "                               capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'CPU Affinity' in line or 'NUMA Affinity' in line:\n",
    "                    # è§£æNUMAä¿¡æ¯\n",
    "                    for i, gpu_line in enumerate(lines):\n",
    "                        if i > 0 and 'GPU' in gpu_line and '\\t' in gpu_line:\n",
    "                            parts = gpu_line.split('\\t')\n",
    "                            if len(parts) >= 6:\n",
    "                                gpu_id = i - 1\n",
    "                                numa_affinity = parts[5] if len(parts) > 5 else 'N/A'\n",
    "                                numa_groups[gpu_id] = numa_affinity\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 4. æ£€æŸ¥PCIeé…ç½®ä¿¡æ¯\n",
    "    print(\"\\n4. PCIeé…ç½®ä¿¡æ¯åˆ†æ:\")\n",
    "    print(\"-\" * 80)\n",
    "    pcie_info = {}\n",
    "    try:\n",
    "        # ä½¿ç”¨lspciè·å–PCIeä¿¡æ¯\n",
    "        result = subprocess.run(['lspci', '-v'], capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.split('\\n')\n",
    "            current_gpu = None\n",
    "            for line in lines:\n",
    "                if 'VGA compatible controller' in line or '3D controller' in line:\n",
    "                    if 'NVIDIA' in line:\n",
    "                        # æå–PCIeåœ°å€\n",
    "                        pcie_addr = line.split()[0]\n",
    "                        current_gpu = pcie_addr\n",
    "                        pcie_info[current_gpu] = {'addr': pcie_addr, 'details': []}\n",
    "                elif current_gpu and ('LnkSta' in line or 'LnkCap' in line or 'PCIe' in line):\n",
    "                    pcie_info[current_gpu]['details'].append(line.strip())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # ä½¿ç”¨nvidia-smiæŸ¥è¯¢PCIeä¿¡æ¯\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=index,pci.bus_id,pcie.link.gen.current,pcie.link.width.current', \n",
    "                                '--format=csv,noheader'], capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            print(\"PCIeé“¾è·¯ä¿¡æ¯:\")\n",
    "            for line in result.stdout.strip().split('\\n'):\n",
    "                parts = line.split(', ')\n",
    "                if len(parts) >= 4:\n",
    "                    gpu_idx = parts[0].strip()\n",
    "                    pci_bus = parts[1].strip()\n",
    "                    pcie_gen = parts[2].strip()\n",
    "                    pcie_width = parts[3].strip()\n",
    "                    print(f\"  GPU{gpu_idx}: PCIeæ€»çº¿={pci_bus}, ç‰ˆæœ¬=Gen{pcie_gen}, å®½åº¦=x{pcie_width}\")\n",
    "                    pcie_info[f'gpu{gpu_idx}'] = {\n",
    "                        'gen': pcie_gen,\n",
    "                        'width': pcie_width,\n",
    "                        'bus': pci_bus\n",
    "                    }\n",
    "    except:\n",
    "        print(\"  æ— æ³•è·å–PCIeè¯¦ç»†ä¿¡æ¯\")\n",
    "    \n",
    "    # 5. è¯¦ç»†åˆ†æï¼ˆæ— NVLinkæƒ…å†µï¼‰\n",
    "    print(\"\\n5. æ— NVLinkæƒ…å†µä¸‹çš„é€Ÿåº¦å·®å¼‚åˆ†æ:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"æ‚¨çš„ç³»ç»Ÿæ‹“æ‰‘ç»“æ„:\")\n",
    "    print(\"  â€¢ GPU0å’ŒGPU1: åŒä¸€NUMAèŠ‚ç‚¹ï¼ˆèŠ‚ç‚¹0ï¼‰ï¼Œæœ‰P2Pæ”¯æŒ\")\n",
    "    print(\"  â€¢ GPU2å’ŒGPU3: åŒä¸€NUMAèŠ‚ç‚¹ï¼ˆèŠ‚ç‚¹1ï¼‰ï¼Œæœ‰P2Pæ”¯æŒ\")\n",
    "    print(\"  â€¢ ç³»ç»Ÿæ— NVLinkè¿æ¥\")\n",
    "    print()\n",
    "    print(\"é€Ÿåº¦å·®å¼‚çš„æ ¹æœ¬åŸå› ï¼ˆæ— NVLinkï¼‰:\")\n",
    "    print()\n",
    "    \n",
    "    # æ£€æŸ¥GPU0å’ŒGPU1çš„P2P\n",
    "    if p2p_matrix.get((0, 1), False) and p2p_matrix.get((1, 0), False):\n",
    "        print(\"  âœ“ GPU0 â†” GPU1: é€Ÿåº¦å¿«ï¼ˆ~1000+ GB/sï¼‰\")\n",
    "        print(\"    â†’ æœ‰P2Pæ”¯æŒï¼Œé€šè¿‡PCIe P2Pé€šä¿¡\")\n",
    "        print(\"    â†’ å¯èƒ½çš„åŸå› :\")\n",
    "        print(\"       1. åŒä¸€PCIeæ ¹å¤åˆä½“ï¼ˆRoot Complexï¼‰ä¸‹\")\n",
    "        print(\"       2. é€šè¿‡PCIeäº¤æ¢æœºç›´æ¥è¿æ¥ï¼ˆPIXè¿æ¥ï¼‰\")\n",
    "        print(\"       3. PCIe 4.0 x16é…ç½®ï¼Œç†è®ºå¸¦å®½64 GB/sï¼ˆåŒå‘128 GB/sï¼‰\")\n",
    "        print(\"       4. æ•°æ®ç›´æ¥åœ¨PCIeåŸŸå†…ä¼ è¾“ï¼Œæ— éœ€ç»è¿‡CPUå†…å­˜\")\n",
    "        print(\"    â†’ è™½ç„¶æ— NVLinkï¼Œä½†PCIe P2Pä»ç„¶å¯ä»¥å®ç°è¾ƒé«˜é€Ÿåº¦\")\n",
    "    else:\n",
    "        print(\"  âš  GPU0å’ŒGPU1ä¹‹é—´æ²¡æœ‰P2Pæ”¯æŒ\")\n",
    "    \n",
    "    print()\n",
    "    if p2p_matrix.get((2, 3), False) and p2p_matrix.get((3, 2), False):\n",
    "        print(\"  âš  GPU2 â†” GPU3: é€Ÿåº¦è¾ƒæ…¢ï¼ˆå³ä½¿æœ‰P2Pæ”¯æŒï¼‰\")\n",
    "        print(\"    â†’ è™½ç„¶æœ‰P2Pæ”¯æŒï¼Œä½†é€Ÿåº¦ä»ç„¶è¾ƒæ…¢ï¼Œå¯èƒ½çš„åŸå› :\")\n",
    "        print(\"       1. ä¸åŒçš„PCIeæ ¹å¤åˆä½“é…ç½®\")\n",
    "        print(\"       2. PCIeç‰ˆæœ¬è¾ƒä½ï¼ˆå¦‚PCIe 3.0 vs 4.0ï¼‰\")\n",
    "        print(\"       3. PCIeé€šé“æ•°è¾ƒå°‘ï¼ˆå¦‚x8 vs x16ï¼‰\")\n",
    "        print(\"       4. éœ€è¦é€šè¿‡é¢å¤–çš„PCIeæ¡¥æ¥èŠ¯ç‰‡ï¼Œå¢åŠ å»¶è¿Ÿ\")\n",
    "        print(\"       5. PCIeæ’æ§½ç±»å‹ä¸åŒï¼ˆå¦‚x16ç‰©ç†ä½†x8ç”µæ°”ï¼‰\")\n",
    "        print(\"    â†’ æ£€æŸ¥PCIeé…ç½®:\")\n",
    "        if 'gpu2' in pcie_info and 'gpu3' in pcie_info:\n",
    "            gpu2_gen = pcie_info.get('gpu2', {}).get('gen', 'N/A')\n",
    "            gpu2_width = pcie_info.get('gpu2', {}).get('width', 'N/A')\n",
    "            gpu3_gen = pcie_info.get('gpu3', {}).get('gen', 'N/A')\n",
    "            gpu3_width = pcie_info.get('gpu3', {}).get('width', 'N/A')\n",
    "            print(f\"       GPU2: PCIe Gen{gpu2_gen} x{gpu2_width}\")\n",
    "            print(f\"       GPU3: PCIe Gen{gpu3_gen} x{gpu3_width}\")\n",
    "        if 'gpu0' in pcie_info and 'gpu1' in pcie_info:\n",
    "            gpu0_gen = pcie_info.get('gpu0', {}).get('gen', 'N/A')\n",
    "            gpu0_width = pcie_info.get('gpu0', {}).get('width', 'N/A')\n",
    "            gpu1_gen = pcie_info.get('gpu1', {}).get('gen', 'N/A')\n",
    "            gpu1_width = pcie_info.get('gpu1', {}).get('width', 'N/A')\n",
    "            print(f\"       GPU0: PCIe Gen{gpu0_gen} x{gpu0_width}\")\n",
    "            print(f\"       GPU1: PCIe Gen{gpu1_gen} x{gpu1_width}\")\n",
    "            print()\n",
    "            print(\"    â†’ å¯¹æ¯”åˆ†æ:\")\n",
    "            if gpu0_gen != 'N/A' and gpu2_gen != 'N/A':\n",
    "                if gpu0_gen > gpu2_gen:\n",
    "                    print(f\"       GPU0/1ä½¿ç”¨PCIe Gen{gpu0_gen}ï¼Œè€ŒGPU2/3ä½¿ç”¨PCIe Gen{gpu2_gen}\")\n",
    "                    print(f\"       PCIe Gen{gpu0_gen}å¸¦å®½æ˜¯Gen{gpu2_gen}çš„2å€\")\n",
    "                elif gpu0_width != 'N/A' and gpu2_width != 'N/A':\n",
    "                    if int(gpu0_width.replace('x', '')) > int(gpu2_width.replace('x', '')):\n",
    "                        print(f\"       GPU0/1ä½¿ç”¨x{gpu0_width}ï¼Œè€ŒGPU2/3ä½¿ç”¨x{gpu2_width}\")\n",
    "                        print(f\"       æ›´å¤šé€šé“æ•°æ„å‘³ç€æ›´é«˜å¸¦å®½\")\n",
    "    else:\n",
    "        print(\"  âœ— GPU2 â†” GPU3: æ— P2Pæ”¯æŒ\")\n",
    "        print(\"    â†’ å¿…é¡»é€šè¿‡CPUå†…å­˜ä¸­è½¬ï¼Œé€Ÿåº¦å¾ˆæ…¢\")\n",
    "    \n",
    "    print()\n",
    "    print(\"  âš  GPU0/1 â†” GPU2/3: 1-15 GB/sï¼ˆè·¨NUMAèŠ‚ç‚¹é€šä¿¡ï¼‰\")\n",
    "    print(\"    â†’ æ— P2Pæ”¯æŒï¼Œå¿…é¡»é€šè¿‡CPUå’ŒPCIe\")\n",
    "    print(\"    â†’ éœ€è¦ç»è¿‡ä»¥ä¸‹è·¯å¾„:\")\n",
    "    print(\"       1. GPU0 â†’ PCIe â†’ CPU0å†…å­˜ (NUMAèŠ‚ç‚¹0)\")\n",
    "    print(\"       2. CPU0å†…å­˜ â†’ QPI/UPIäº’è¿ â†’ CPU1å†…å­˜ (NUMAèŠ‚ç‚¹1)\")\n",
    "    print(\"       3. CPU1å†…å­˜ â†’ PCIe â†’ GPU2\")\n",
    "    print(\"    â†’ è¿™ä¸ªè·¯å¾„æ¶‰åŠå¤šæ¬¡æ•°æ®æ‹·è´å’ŒCPUå‚ä¸ï¼Œå¯¼è‡´:\")\n",
    "    print(\"       â€¢ é«˜å»¶è¿Ÿï¼ˆå¤šæ¬¡å†…å­˜æ‹·è´ï¼‰\")\n",
    "    print(\"       â€¢ ä½å¸¦å®½ï¼ˆå—CPUäº’è¿å¸¦å®½é™åˆ¶ï¼Œé€šå¸¸<20 GB/sï¼‰\")\n",
    "    print(\"    â†’ è¿™æ˜¯å…¸å‹çš„NUMAæ¶æ„é™åˆ¶\")\n",
    "    \n",
    "    # 6. æ£€æŸ¥GPUå±æ€§\n",
    "    print(\"\\n6. GPUè®¾å¤‡å±æ€§:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i in range(num_gpus):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU{i}:\")\n",
    "        print(f\"    åç§°: {props.name}\")\n",
    "        print(f\"    PCIæ€»çº¿ID: {props.pci_bus_id if hasattr(props, 'pci_bus_id') else 'N/A'}\")\n",
    "        print(f\"    PCIè®¾å¤‡ID: {props.pci_device_id if hasattr(props, 'pci_device_id') else 'N/A'}\")\n",
    "        print(f\"    å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}\")\n",
    "    \n",
    "    # 7. ä¼˜åŒ–å»ºè®®ï¼ˆæ— NVLinkæƒ…å†µï¼‰\n",
    "    print(\"\\n7. é’ˆå¯¹æ— NVLinkç³»ç»Ÿçš„ä¼˜åŒ–å»ºè®®:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"åŸºäºNUMAæ‹“æ‰‘çš„ä¼˜åŒ–ç­–ç•¥:\")\n",
    "    print()\n",
    "    print(\"  1. æ•°æ®å¹¶è¡Œè®­ç»ƒ:\")\n",
    "    print(\"     â€¢ å°†æ¨¡å‹å‰¯æœ¬æ”¾åœ¨åŒä¸€NUMAèŠ‚ç‚¹å†…ï¼ˆGPU0+GPU1 æˆ– GPU2+GPU3ï¼‰\")\n",
    "    print(\"     â€¢ é¿å…è·¨NUMAèŠ‚ç‚¹çš„é¢‘ç¹é€šä¿¡\")\n",
    "    print(\"     â€¢ ä½¿ç”¨DataParallelæ—¶ï¼Œè®¾ç½®device_ids=[0,1]æˆ–[2,3]\")\n",
    "    print()\n",
    "    print(\"  2. æ¨¡å‹å¹¶è¡Œè®­ç»ƒ:\")\n",
    "    print(\"     â€¢ å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼Œå°†ä¸åŒå±‚æ”¾åœ¨åŒä¸€NUMAèŠ‚ç‚¹çš„GPUä¸Š\")\n",
    "    print(\"     â€¢ ä¾‹å¦‚ï¼šå‰å‡ å±‚åœ¨GPU0ï¼Œåå‡ å±‚åœ¨GPU1ï¼ˆéƒ½åœ¨èŠ‚ç‚¹0ï¼‰\")\n",
    "    print(\"     â€¢ é¿å…å°†æ¨¡å‹åˆ†å‰²åˆ°è·¨NUMAèŠ‚ç‚¹çš„GPUä¸Š\")\n",
    "    print()\n",
    "    print(\"  3. æ··åˆç²¾åº¦å’Œé€šä¿¡ä¼˜åŒ–:\")\n",
    "    print(\"     â€¢ ä½¿ç”¨æ¢¯åº¦å‹ç¼©ï¼ˆå¦‚FP16ï¼‰å‡å°‘è·¨NUMAé€šä¿¡çš„æ•°æ®é‡\")\n",
    "    print(\"     â€¢ ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘é€šä¿¡é¢‘ç‡\")\n",
    "    print(\"     â€¢ NCCLä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è·¯å¾„ï¼Œä½†è·¨NUMAä»ç„¶å¾ˆæ…¢\")\n",
    "    print()\n",
    "    print(\"  4. è¿›ç¨‹ç»‘å®š:\")\n",
    "    print(\"     â€¢ ä½¿ç”¨numactlå°†è¿›ç¨‹ç»‘å®šåˆ°ç‰¹å®šNUMAèŠ‚ç‚¹\")\n",
    "    print(\"     â€¢ ä¾‹å¦‚ï¼šnumactl --cpunodebind=0 --membind=0 python train.py\")\n",
    "    print(\"     â€¢ è¿™å¯ä»¥é¿å…è·¨NUMAèŠ‚ç‚¹çš„å†…å­˜è®¿é—®\")\n",
    "    print()\n",
    "    print(\"  5. å¦‚æœå¿…é¡»ä½¿ç”¨æ‰€æœ‰4ä¸ªGPU:\")\n",
    "    print(\"     â€¢ è€ƒè™‘ä½¿ç”¨2ä¸ªç‹¬ç«‹çš„è®­ç»ƒè¿›ç¨‹ï¼ˆæ¯ä¸ªä½¿ç”¨2ä¸ªGPUï¼‰\")\n",
    "    print(\"     â€¢ æˆ–è€…æ¥å—è·¨NUMAé€šä¿¡çš„æ€§èƒ½æŸå¤±\")\n",
    "    print(\"     â€¢ ä½¿ç”¨æ›´å¤§çš„batch sizeå’Œæ›´å°‘çš„é€šä¿¡é¢‘ç‡æ¥è¡¥å¿\")\n",
    "    \n",
    "    # 8. PCIe P2Pæ€§èƒ½è¯´æ˜\n",
    "    print(\"\\n8. PCIe P2Pæ€§èƒ½è¯´æ˜:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"PCIe P2Pï¼ˆPeer-to-Peerï¼‰é€šä¿¡ç‰¹ç‚¹:\")\n",
    "    print(\"  â€¢ å…è®¸GPUä¹‹é—´ç›´æ¥é€šè¿‡PCIeé€šä¿¡ï¼Œæ— éœ€ç»è¿‡CPUå†…å­˜\")\n",
    "    print(\"  â€¢ æ€§èƒ½å–å†³äºPCIeé…ç½®:\")\n",
    "    print(\"     - PCIe 4.0 x16: ç†è®ºå•å‘32 GB/sï¼ŒåŒå‘64 GB/s\")\n",
    "    print(\"     - PCIe 3.0 x16: ç†è®ºå•å‘16 GB/sï¼ŒåŒå‘32 GB/s\")\n",
    "    print(\"     - PCIe 4.0 x8:  ç†è®ºå•å‘16 GB/sï¼ŒåŒå‘32 GB/s\")\n",
    "    print(\"     - PCIe 3.0 x8:  ç†è®ºå•å‘8 GB/sï¼ŒåŒå‘16 GB/s\")\n",
    "    print(\"  â€¢ å®é™…æµ‹è¯•é€Ÿåº¦å¯èƒ½æ¥è¿‘ç†è®ºå€¼ï¼ˆå¦‚æœé…ç½®æ­£ç¡®ï¼‰\")\n",
    "    print(\"  â€¢ GPU0 â†” GPU1é€Ÿåº¦å¿«ï¼Œè¯´æ˜PCIe P2Pé…ç½®è‰¯å¥½\")\n",
    "    print(\"  â€¢ GPU2 â†” GPU3é€Ÿåº¦æ…¢ï¼Œå¯èƒ½æ˜¯PCIeé…ç½®ä¸åŒæˆ–å—é™\")\n",
    "    print()\n",
    "    print(\"æ£€æŸ¥PCIeé…ç½®å·®å¼‚çš„æ–¹æ³•:\")\n",
    "    print(\"  â€¢ è¿è¡Œ: nvidia-smi --query-gpu=index,pcie.link.gen.current,pcie.link.width.current --format=csv\")\n",
    "    print(\"  â€¢ æ£€æŸ¥BIOSä¸­çš„PCIeé…ç½®\")\n",
    "    print(\"  â€¢ ç¡®è®¤æ‰€æœ‰GPUæ’æ§½éƒ½é…ç½®ä¸ºx16ï¼ˆå¦‚æœä¸»æ¿æ”¯æŒï¼‰\")\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨nvidia-ml-pyè·å–æ›´è¯¦ç»†çš„ä¿¡æ¯\n",
    "        try:\n",
    "            import pynvml\n",
    "            pynvml.nvmlInit()\n",
    "            for i in range(num_gpus):\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                try:\n",
    "                    # è·å–NVLinkä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "                    nvlink_version = pynvml.nvmlDeviceGetNvLinkVersion(handle, 0)\n",
    "                    print(f\"  GPU{i} NVLinkç‰ˆæœ¬: {nvlink_version}\")\n",
    "                except:\n",
    "                    pass\n",
    "            pynvml.nvmlShutdown()\n",
    "        except ImportError:\n",
    "            print(\"  æç¤º: å®‰è£…pynvmlå¯ä»¥è·å–æ›´è¯¦ç»†çš„NVLinkä¿¡æ¯\")\n",
    "            print(\"        å‘½ä»¤: pip install nvidia-ml-py3\")\n",
    "        except Exception as e:\n",
    "            print(f\"  æ— æ³•è·å–NVLinkä¿¡æ¯: {e}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å¯ç”¨çš„GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
